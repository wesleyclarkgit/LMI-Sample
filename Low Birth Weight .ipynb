{"nbformat_minor": 1, "cells": [{"source": "# Predict Survival of the Titanic Passengers", "cell_type": "markdown", "metadata": {}}, {"source": "Dataset Source: [https://ww2.amstat.org/publications/jse/v3n3/datasets.dawson.html](https://ww2.amstat.org/publications/jse/v3n3/datasets.dawson.html)", "cell_type": "markdown", "metadata": {}}, {"source": "## Table of Contents\n- [Load Libraries](#load_libraries)\n- [Access data](#access_data)\n- [Split Data into Training and Test set](#training_test)\n- [Build Logistic Regression Model](#build_model)\n- [Predict for Test data](#test_data)\n- [Evaluate the Model](#evaluate_model)", "cell_type": "markdown", "metadata": {}}, {"source": "<a id=\"load_libraries\"></a>\n## Load Libraries\n\nThe Spark and Python libraries that you need are preinstalled in the notebook environment and only need to be loaded.\n\nRun the following cell to load the libraries you will work with in this notebook:", "cell_type": "markdown", "metadata": {}}, {"source": "# PySpark Machine Learning Library\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.feature import HashingTF, Tokenizer\nfrom pyspark.sql import Row, SQLContext\n\nimport os\nimport sys\nfrom pyspark import SparkConf\nfrom pyspark import SparkContext\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql.types import *\n\nfrom pyspark.mllib.classification import LogisticRegressionWithSGD\nfrom pyspark.mllib.regression import LabeledPoint\nfrom numpy import array\n\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n# Library for confusion matrix, precision, test error\nfrom pyspark.mllib.evaluation import MulticlassMetrics\n# Library For Area under ROC curve and Area under precision-recall curve\nfrom pyspark.mllib.evaluation import BinaryClassificationMetrics\n\n# Assign resources to the application\nsqlContext = SQLContext(sc)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 18}, {"source": "# The data will be loaded into an array.\n# This is the summary of the data structure, including the column position and name.\n# The first filed starts from position 0. \n\n# 0 ID    -  Identification Code.\n# 1 LOW  -  Low birth weight (0: >= 2500, 1: < 2500 g) -  target variable\n# 2 Age - Mother's age in years\n# 3 Race - Race (1: White, 2: Black, 3: Other)\n# 4 Smoke - Smoking status during pregnancy\n# 5 PTL - History of premature labor (1: None, 2: One, 3: Two, etc)\n# 6 HT - History of hypertension(1: No, 2: Yes)\n# 7 UI - Presence of Uterine irritability (1: No, 2: Yes)\n# 8 FTV - Number of physician visits during the first trimester (1: None, 2: One, 3: Two, etc)\n\n# Label is a target variable. PersonInfo is a list of independent variables besides unique identifier\n\nLabeledDocument = Row(\"PersonID\", \"PersonInfo\", \"label\")\n\n# Define a function that parses the raw CSV file and returns an object of type LabeledDocument\n\ndef parseDocument(line):\n    values = [str(x) for x in line.split(',')] \n    if (values[1]<'2500'):\n      Low = 1.0\n    else:\n      Low = 0.0\n        \n    textValue = str(values[1]) + \" \" + str(values[2])+\" \" + str(values[3])\n    return LabeledDocument(values[5], textValue, Low)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 19}, {"source": "<a id=\"access_data\"></a>\n## Access Data\nBefore you can access data in the data file in the Object Storage, you must setup the Spark configuration with your Object Storage credentials. \n\nTo do this, click on the cell below and select the **Insert to code > Insert Spark Session DataFrame** function from the Files tab below the data file you want to work with.", "cell_type": "markdown", "metadata": {}}, {"source": "<div class=\"alert alert-block alert-info\">The following code contains the credentials for a file in your IBM Cloud Object Storage. Delete the code starting from `from pyspark.sql import SparkSession` line before you run the cell.</div>", "cell_type": "markdown", "metadata": {}}, {"source": "# Object Storage Credentials\nimport ibmos2spark\n\n# @hidden_cell\ncredentials = {\n    'endpoint': 'https://s3-api.us-geo.objectstorage.service.networklayer.com',\n    'api_key': 'JVTHf52ulbECLgcNi_HtACeV8GS2PPzazxUhftWUrxlS',\n    'service_id': 'iam-ServiceId-c3f94a6d-ea3d-4889-aba0-2d5f40560f3c',\n    'iam_service_endpoint': 'https://iam.ng.bluemix.net/oidc/token'}\n\nconfiguration_name = 'os_31f75a6160394f40bab252e191667cc8_configs'\ncos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\n", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 20}, {"source": "Now let's load the data into a `Spark RDD` and output the number of rows and first 5 rows.\nEach project you create has a bucket in your object storage. You can get the bucket name from the project Settings page. Change the string `BUCKET` to the bucket name", "cell_type": "markdown", "metadata": {}}, {"source": "data = sc.textFile(cos.url('lowbwt.csv', 'wesleyclarkassignment3lowbirthwei-donotdelete-pr-rli3ie5o1dyvdn'))\nprint \"Total records in the data set:\", data.count()\nprint \"The first 5 rows\"\ndata.take(5)", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "Total records in the data set: 190\nThe first 5 rows\n"}, {"output_type": "execute_result", "data": {"text/plain": "[u'ID,LOW,AGE,RACE,SMOKE,PTL,HT,UI,FTV',\n u'85,0,19,2,0,0,0,1,0',\n u'86,0,33,3,0,0,0,0,3',\n u'87,0,20,1,1,0,0,0,1',\n u'88,0,21,1,1,0,0,1,2']"}, "execution_count": 21, "metadata": {}}], "execution_count": 21}, {"source": "Crate DataFrame from RDD", "cell_type": "markdown", "metadata": {}}, {"source": "#Load the data into a dataframe, parse it using the function above\ndocuments = data.filter(lambda s: \"ID\" not in s).map(parseDocument)\nlowbwtData = documents.toDF() # ToDataFrame\nprint \"Number of records: \" + str(lowbwtData.count())\nprint \"First 5 records: \"\nlowbwtData.take(5)", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "Number of records: 189\nFirst 5 records: \n"}, {"output_type": "execute_result", "data": {"text/plain": "[Row(PersonID=u'0', PersonInfo=u'0 19 2', label=1.0),\n Row(PersonID=u'0', PersonInfo=u'0 33 3', label=1.0),\n Row(PersonID=u'0', PersonInfo=u'0 20 1', label=1.0),\n Row(PersonID=u'0', PersonInfo=u'0 21 1', label=1.0),\n Row(PersonID=u'0', PersonInfo=u'0 18 1', label=1.0)]"}, "execution_count": 22, "metadata": {}}], "execution_count": 22}, {"source": "<a id=\"training_test\"></a>\n## Split Data into Training and Test set\n\nWe divide the data into training and test set.  The training set is used to build the model to be used on future data, and the test set is used to evaluate the model.", "cell_type": "markdown", "metadata": {}}, {"source": "# Divide the data into training and test set\n(train, test) = lowbwtData.randomSplit([0.8, 0.2])\nprint \"Number of records in the training set: \" + str(train.count())\nprint \"Number of records in the test set: \" + str(test.count())\n# Output first 20 records in the training set\nprint \"First 20 records in the training set: \"\ntrain.show()", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "Number of records in the training set: 162\nNumber of records in the test set: 27\nFirst 20 records in the training set: \n+--------+----------+-----+\n|PersonID|PersonInfo|label|\n+--------+----------+-----+\n|       0|    0 15 2|  1.0|\n|       0|    0 16 1|  1.0|\n|       0|    0 16 2|  1.0|\n|       0|    0 16 3|  1.0|\n|       0|    0 17 2|  1.0|\n|       0|    0 17 2|  1.0|\n|       0|    0 17 3|  1.0|\n|       0|    0 17 3|  1.0|\n|       0|    0 17 3|  1.0|\n|       0|    0 18 1|  1.0|\n|       0|    0 18 1|  1.0|\n|       0|    0 18 1|  1.0|\n|       0|    0 18 1|  1.0|\n|       0|    0 18 2|  1.0|\n|       0|    0 19 1|  1.0|\n|       0|    0 19 1|  1.0|\n|       0|    0 19 2|  1.0|\n|       0|    0 19 3|  1.0|\n|       0|    0 19 3|  1.0|\n|       0|    0 19 3|  1.0|\n+--------+----------+-----+\nonly showing top 20 rows\n\n"}], "execution_count": 23}, {"source": "<a id=\"build_model\"></a>\n## Build Logistic Regression Model\n\nWe use the Pipeline of SparkML to build the Logistic Regression Model", "cell_type": "markdown", "metadata": {}}, {"source": "# set up Logistic Regression using Pipeline of SparkML\ntokenizer = Tokenizer(inputCol=\"PersonInfo\", outputCol=\"words\")\nhashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\nlr = LogisticRegression(maxIter=10, regParam=0.01)\npipeline = Pipeline(stages=[tokenizer, hashingTF, lr])", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 24}, {"source": "# set up Logistic Regression Model\n# the stages are executed in order\nmodel = pipeline.fit(train)\n#[stage.coefficients for stage in model.stages if hasattr(stage, \"coefficients\")]\n# model.stages[2].intercept\nmodel.stages[2].coefficients", "cell_type": "code", "metadata": {"scrolled": true}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "SparseVector(262144, {})"}, "execution_count": 25, "metadata": {}}], "execution_count": 25}, {"source": "<a id=\"test_data\"></a>\n## Predict for Test data", "cell_type": "markdown", "metadata": {}}, {"source": "# Make predictions on test documents and print columns of interest\nprediction = model.transform(test)\nselected = prediction.select(\"PersonInfo\", \"prediction\", \"probability\")\nfor row in selected.collect():\n    print row\n#for row in prediction.collect():\n#    print row", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "Row(PersonInfo=u'0 17 1', prediction=1.0, probability=DenseVector([0.0, 1.0]))\nRow(PersonInfo=u'0 17 3', prediction=1.0, probability=DenseVector([0.0, 1.0]))\nRow(PersonInfo=u'0 18 1', prediction=1.0, probability=DenseVector([0.0, 1.0]))\nRow(PersonInfo=u'0 19 1', prediction=1.0, probability=DenseVector([0.0, 1.0]))\nRow(PersonInfo=u'0 19 3', prediction=1.0, probability=DenseVector([0.0, 1.0]))\nRow(PersonInfo=u'0 21 1', prediction=1.0, probability=DenseVector([0.0, 1.0]))\nRow(PersonInfo=u'0 22 1', prediction=1.0, probability=DenseVector([0.0, 1.0]))\nRow(PersonInfo=u'0 22 1', prediction=1.0, probability=DenseVector([0.0, 1.0]))\nRow(PersonInfo=u'0 23 1', prediction=1.0, probability=DenseVector([0.0, 1.0]))\nRow(PersonInfo=u'0 28 3', prediction=1.0, probability=DenseVector([0.0, 1.0]))\nRow(PersonInfo=u'0 32 3', prediction=1.0, probability=DenseVector([0.0, 1.0]))\nRow(PersonInfo=u'0 33 1', prediction=1.0, probability=DenseVector([0.0, 1.0]))\nRow(PersonInfo=u'0 36 1', prediction=1.0, probability=DenseVector([0.0, 1.0]))\nRow(PersonInfo=u'0 28 3', prediction=1.0, probability=DenseVector([0.0, 1.0]))\nRow(PersonInfo=u'0 30 1', prediction=1.0, probability=DenseVector([0.0, 1.0]))\nRow(PersonInfo=u'0 33 1', prediction=1.0, probability=DenseVector([0.0, 1.0]))\nRow(PersonInfo=u'0 45 1', prediction=1.0, probability=DenseVector([0.0, 1.0]))\nRow(PersonInfo=u'1 17 1', prediction=1.0, probability=DenseVector([0.0, 1.0]))\nRow(PersonInfo=u'1 18 3', prediction=1.0, probability=DenseVector([0.0, 1.0]))\nRow(PersonInfo=u'1 20 3', prediction=1.0, probability=DenseVector([0.0, 1.0]))\nRow(PersonInfo=u'1 23 3', prediction=1.0, probability=DenseVector([0.0, 1.0]))\nRow(PersonInfo=u'1 26 1', prediction=1.0, probability=DenseVector([0.0, 1.0]))\nRow(PersonInfo=u'1 27 3', prediction=1.0, probability=DenseVector([0.0, 1.0]))\nRow(PersonInfo=u'1 29 1', prediction=1.0, probability=DenseVector([0.0, 1.0]))\nRow(PersonInfo=u'1 18 2', prediction=1.0, probability=DenseVector([0.0, 1.0]))\nRow(PersonInfo=u'1 21 3', prediction=1.0, probability=DenseVector([0.0, 1.0]))\nRow(PersonInfo=u'1 26 1', prediction=1.0, probability=DenseVector([0.0, 1.0]))\n"}], "execution_count": 26}, {"source": "#Tabulate the predicted outcome\nprediction.select(\"prediction\").groupBy(\"prediction\").count().show(truncate=False)", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "+----------+-----+\n|prediction|count|\n+----------+-----+\n|1.0       |27   |\n+----------+-----+\n\n"}], "execution_count": 27}, {"source": "#Tabulate the actual outcome\nprediction.select(\"label\").groupBy(\"label\").count().show(truncate=False)", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-----+-----+\n|label|count|\n+-----+-----+\n|1.0  |27   |\n+-----+-----+\n\n"}], "execution_count": 28}, {"source": "# This table shows:\n# 1. The number of survived passengers predicted as survived\n# 2. The number of survived passengers predicted as not survived\n# 3. The number of not survived passengers predicted as survived\n# 4. The number of not survived passengers predicted as not survived\n\nprediction.crosstab('label', 'prediction').show()", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "+----------------+---+\n|label_prediction|1.0|\n+----------------+---+\n|             1.0| 27|\n+----------------+---+\n\n"}], "execution_count": 29}, {"source": "<a id=\"evaluate_model\"></a>\n## Evaluate the Model\n\nWe evaluate the model on a training set and on a test set.  The purpose is to measure the model's predictive accuracy, including the accuracy for new data.", "cell_type": "markdown", "metadata": {}}, {"source": "# Evaluate the Logistic Regression model on a training set\n# Select (prediction, true label) and compute test error\npred_lr=model.transform(train).select(\"prediction\", \"label\")\neval_lr=MulticlassClassificationEvaluator (\n    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy_lr=eval_lr.evaluate(pred_lr)\n# create RDD\npredictionAndLabels_lr=pred_lr.rdd\nmetrics_lr=MulticlassMetrics(predictionAndLabels_lr)\nprecision_lr=metrics_lr.precision(1.0)\nrecall_lr=metrics_lr.recall(1.0)\nf1Measure_lr = metrics_lr.fMeasure(1.0, 1.0)\nprint(\"F1 Measure = %s\" % f1Measure_lr)\nprint (\"Test Accuracy = %s\" %accuracy_lr)\nprint (\"Test Error = %s\" % (1-accuracy_lr))\nprint (\"Precision = %s\" %precision_lr)\nprint (\"Recall = %s\" %recall_lr)", "cell_type": "code", "metadata": {"scrolled": true}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "F1 Measure = 1.0\nTest Accuracy = 1.0\nTest Error = 0.0\nPrecision = 1.0\nRecall = 1.0\n"}], "execution_count": 30}, {"source": "# Evaluate the Logistic Regression model on a test set\n# Select (prediction, true label) and compute test error\npred_lr=model.transform(test).select(\"prediction\", \"label\")\neval_lr=MulticlassClassificationEvaluator (\n    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy_lr=eval_lr.evaluate(pred_lr)\n# create RDD\npredictionAndLabels_lr=pred_lr.rdd\nmetrics_lr=MulticlassMetrics(predictionAndLabels_lr)\nprecision_lr=metrics_lr.precision(1.0)\nrecall_lr=metrics_lr.recall(1.0)\nf1Measure_lr = metrics_lr.fMeasure(1.0, 1.0)\nprint(\"F1 Measure = %s\" % f1Measure_lr)\nprint (\"Test Accuracy = %s\" %accuracy_lr)\nprint (\"Test Error = %s\" % (1-accuracy_lr))\nprint (\"Precision = %s\" %precision_lr)\nprint (\"Recall = %s\" %recall_lr)", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "F1 Measure = 1.0\nTest Accuracy = 1.0\nTest Error = 0.0\nPrecision = 1.0\nRecall = 1.0\n"}], "execution_count": 31}, {"source": "bin_lr=BinaryClassificationMetrics(predictionAndLabels_lr)\n\n# Area under precision-recall curve\nprint(\"Area under PR = %s\" % bin_lr.areaUnderPR)\n# Area under precision-recall curve\nprint(\"Area under ROC = %s\" % bin_lr.areaUnderROC)", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "Area under PR = 1.0\nArea under ROC = 1.0\n"}], "execution_count": 32}, {"source": "", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}], "metadata": {"kernelspec": {"display_name": "Python 2 with Spark 2.1", "name": "python2-spark21", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "2.7.11", "name": "python", "pygments_lexer": "ipython2", "file_extension": ".py", "codemirror_mode": {"version": 2, "name": "ipython"}}, "name": "machineLearningHAVCBluemix.ipynb"}, "nbformat": 4}